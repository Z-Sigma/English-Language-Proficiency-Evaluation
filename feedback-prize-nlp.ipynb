{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# DeBERTa LLRD + LastLayerReinit with TensorFlow\n\nIt Uses:\n\n* MultilabelStratifiedKFold split of the data\n* HuggingFace DeBERTaV3 pre-trained model finetuning with Tensorflow\n* WeightedLayerPool + MeanPool TensorFlow implementation\n* Layer-wise learning rate decay\n* Last layer reinitialization or partially reinitialzation","metadata":{}},{"cell_type":"markdown","source":"# Imports","metadata":{}},{"cell_type":"code","source":"import os, gc\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport tensorflow as tf\nprint(f'TF version: {tf.__version__}')\nimport tensorflow_addons as tfa\nfrom tensorflow.keras import layers\n\nimport transformers\nprint(f'transformers version: {transformers.__version__}')\nfrom transformers import logging as hf_logging\nhf_logging.set_verbosity_error()\n\nimport sys\nsys.path.append('../input/iterativestratification')\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def set_seed(seed=42):\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n#   os.environ['TF_DETERMINISTIC_OPS'] = '1'\nset_seed(42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load DataFrame","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('../input/feedback-prize-english-language-learning/train.csv')\ndisplay(df.head())\nprint('\\n---------DataFrame Summary---------')\ndf.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport plotly.express as px #graphing\nimport plotly.graph_objects as go #graphing\nfrom plotly.subplots import make_subplots #graphing\nimport plotly.figure_factory as ff #graphing","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"colormap = sns.color_palette('Blues')\nsns.heatmap(df.corr(), annot = True, cmap = colormap )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = px.histogram(df, x = df['full_text'].str.len(), marginal = 'box', title  = 'Histogram of full_text text length', \n                  color_discrete_sequence = [\"#FFA200\"] )\n\nfig.update_layout(bargap = 0.2)\n\nfig.update_layout(template = 'plotly_dark', font = dict(family = 'PT Sans', size = 19, color = \"#C4FEFF\"  ) )\n\nfig.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"colors = [[\"#00E600\"], [\"#0000E6\"], [\"#E600DF\"], [\"#E6E600\"], [\"#FFFFFF\"], [\"#cd040b\"]]\n\n\nfor count, x in enumerate([\"cohesion\", \"syntax\", \"vocabulary\", \"phraseology\", \"grammar\", \"conventions\"]):\n    fig = px.histogram(df, x = x, marginal = 'violin', title = f\" {x} histogram\", color_discrete_sequence = colors[count]  )\n    \n    fig.update_layout(bargap = 0.2)\n    fig.update_layout(template = 'plotly_dark', font = dict(family = 'PT Sans', size = 19, color = \"#C4FEFF\" ) )\n    \n    fig.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# CV Split","metadata":{}},{"cell_type":"code","source":"N_FOLD = 5\nTARGET_COLS = ['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions']\n\nskf = MultilabelStratifiedKFold(n_splits=N_FOLD, shuffle=True, random_state=42)\nfor n, (train_index, val_index) in enumerate(skf.split(df, df[TARGET_COLS])):\n    df.loc[val_index, 'fold'] = int(n)\ndf['fold'] = df['fold'].astype(int)\ndf['fold'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.to_csv('./df_folds.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Config","metadata":{}},{"cell_type":"code","source":"MAX_LENGTH = 512\nBATCH_SIZE = 14\n\nDEBERTA_MODEL = \"../input/debertav3base\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Why we should disable dropout in regression task, check this [discussion](https://www.kaggle.com/competitions/commonlitreadabilityprize/discussion/260729).","metadata":{}},{"cell_type":"code","source":"tokenizer = transformers.AutoTokenizer.from_pretrained(DEBERTA_MODEL)\ntokenizer.save_pretrained('./tokenizer/')\n\ncfg = transformers.AutoConfig.from_pretrained(DEBERTA_MODEL, output_hidden_states=True)\ncfg.hidden_dropout_prob = 0\ncfg.attention_probs_dropout_prob = 0\ncfg.save_pretrained('./tokenizer/')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Process Function\n\nTo make use of HugggingFace DeBERTa model, we have to tokenize our input texts as the pretrained DeBERTa model requires.","metadata":{}},{"cell_type":"code","source":"def deberta_encode(texts, tokenizer=tokenizer):\n    input_ids = []\n    attention_mask = []\n    for text in texts.tolist():\n        token = tokenizer(text, \n                          add_special_tokens=True, \n                          max_length=MAX_LENGTH, \n                          return_attention_mask=True, \n                          return_tensors=\"np\", \n                          truncation=True, \n                          padding='max_length')\n        input_ids.append(token['input_ids'][0])\n        attention_mask.append(token['attention_mask'][0])\n    \n    return np.array(input_ids, dtype=\"int32\"), np.array(attention_mask, dtype=\"int32\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_dataset(df):\n    inputs = deberta_encode(df['full_text'])\n    targets = np.array(df[TARGET_COLS], dtype=\"float32\")\n    return inputs, targets","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model\n\n## MeanPool\n\nInstead of using '[CLS]' token, MeanPool method averaging one layer of hidden states along the sequence axis with masking out padding tokens.\n\n## WeightedLayerPool\n\nWeightedLayerPool uses a set of trainable weights to average a set of hidden states from transformer backbone. I use a Dense layer with constraint to implement it.","metadata":{}},{"cell_type":"code","source":"class MeanPool(tf.keras.layers.Layer):\n    def call(self, inputs, mask=None):\n        broadcast_mask = tf.expand_dims(tf.cast(mask, \"float32\"), -1)\n        embedding_sum = tf.reduce_sum(inputs * broadcast_mask, axis=1)\n        mask_sum = tf.reduce_sum(broadcast_mask, axis=1)\n        mask_sum = tf.math.maximum(mask_sum, tf.constant([1e-9]))\n        return embedding_sum / mask_sum","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"WeightedLayerPool weights constraints: softmax to push sum(w) to be 1.","metadata":{}},{"cell_type":"code","source":"class WeightsSumOne(tf.keras.constraints.Constraint):\n    def __call__(self, w):\n        return tf.nn.softmax(w, axis=0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model Design Choice\n\nAlthough there are many ways to get your final representations, I choose to take the last 4 layers hidden states of DeBERTa, take MeanPool of them to gather information along the sequence axis, then take WeightedLayerPool with a set of trainable weights to gather information along the depth axis of the model, then finally a regression head.\n\n## Last Layer Reinitialization\n\nReinitialization of the last transformer encoder block: GlorotUniform for Dense kernel, Zeros for Dense bias, Zeros for LayerNorm beta, Ones for LayerNorm gamma.\n\n## Layer-wise Learning Rate Decay\n\nI use MultiOptimizer to implement LLRD: Initial learning rate 1e-5 with layer-wise decay 0.9 for transformer encoder and embedding block, 1e-4 for the rest of the model. All learning rates have ExponentialDecay schedulers with decay rate 0.3","metadata":{}},{"cell_type":"code","source":"def get_model():\n    input_ids = tf.keras.layers.Input(\n        shape=(MAX_LENGTH,), dtype=tf.int32, name=\"input_ids\"\n    )\n    \n    attention_masks = tf.keras.layers.Input(\n        shape=(MAX_LENGTH,), dtype=tf.int32, name=\"attention_masks\"\n    )\n   \n    deberta_model = transformers.TFAutoModel.from_pretrained(DEBERTA_MODEL, config=cfg)\n    \n    #Last Layer Reinitialization or Partially Reinitialization\n#     Uncommon next three lines to check deberta encoder block\n#     print('DeBERTa Encoder Block:')\n#     for layer in deberta_model.deberta.encoder.layer:\n#         print(layer)\n        \n    REINIT_LAYERS = 1\n    normal_initializer = tf.keras.initializers.GlorotUniform()\n    zeros_initializer = tf.keras.initializers.Zeros()\n    ones_initializer = tf.keras.initializers.Ones()\n\n#     print(f'\\nRe-initializing encoder block:')\n    for encoder_block in deberta_model.deberta.encoder.layer[-REINIT_LAYERS:]:\n#         print(f'{encoder_block}')\n        for layer in encoder_block.submodules:\n            if isinstance(layer, tf.keras.layers.Dense):\n                layer.kernel.assign(normal_initializer(shape=layer.kernel.shape, dtype=layer.kernel.dtype))\n                if layer.bias is not None:\n                    layer.bias.assign(zeros_initializer(shape=layer.bias.shape, dtype=layer.bias.dtype))\n\n            elif isinstance(layer, tf.keras.layers.LayerNormalization):\n                layer.beta.assign(zeros_initializer(shape=layer.beta.shape, dtype=layer.beta.dtype))\n                layer.gamma.assign(ones_initializer(shape=layer.gamma.shape, dtype=layer.gamma.dtype))\n\n    deberta_output = deberta_model.deberta(\n        input_ids, attention_mask=attention_masks\n    )\n    hidden_states = deberta_output.hidden_states\n    \n    #WeightedLayerPool + MeanPool of the last 4 hidden states\n    stack_meanpool = tf.stack(\n        [MeanPool()(hidden_s, mask=attention_masks) for hidden_s in hidden_states[-4:]], \n        axis=2)\n    \n    weighted_layer_pool = layers.Dense(1,\n                                       use_bias=False,\n                                       kernel_constraint=WeightsSumOne())(stack_meanpool)\n    \n    weighted_layer_pool = tf.squeeze(weighted_layer_pool, axis=-1)\n    \n    x = layers.Dense(6, activation='sigmoid')(weighted_layer_pool)\n    output = layers.Rescaling(scale=4.0, offset=1.0)(x)\n    model = tf.keras.Model(inputs=[input_ids, attention_masks], outputs=output)\n    \n    #Compile model with Layer-wise Learning Rate Decay\n    layer_list = [deberta_model.deberta.embeddings] + list(deberta_model.deberta.encoder.layer)\n    layer_list.reverse()\n    \n    INIT_LR = 1e-5\n    LLRDR = 0.9\n    LR_SCH_DECAY_STEPS = 1600 # 2 * len(train_df) // BATCH_SIZE\n    \n    lr_schedules = [tf.keras.optimizers.schedules.ExponentialDecay(\n        initial_learning_rate=INIT_LR * LLRDR ** i, \n        decay_steps=LR_SCH_DECAY_STEPS, \n        decay_rate=0.3) for i in range(len(layer_list))]\n    lr_schedule_head = tf.keras.optimizers.schedules.ExponentialDecay(\n        initial_learning_rate=1e-4, \n        decay_steps=LR_SCH_DECAY_STEPS, \n        decay_rate=0.3)\n    \n    optimizers = [tf.keras.optimizers.Adam(learning_rate=lr_sch) for lr_sch in lr_schedules]\n    \n    optimizers_and_layers = [(tf.keras.optimizers.Adam(learning_rate=lr_schedule_head), model.layers[-4:])] +\\\n        list(zip(optimizers, layer_list))\n    \n#     Uncomment next three lines to check optimizers_and_layers\n#     print('\\nLayer-wise Learning Rate Decay Initial LR:')\n#     for o,l in optimizers_and_layers:\n#         print(f'{o._decayed_lr(\"float32\").numpy()} for {l}')\n        \n    optimizer = tfa.optimizers.MultiOptimizer(optimizers_and_layers)\n    \n    model.compile(optimizer=optimizer,\n                 loss='huber_loss',\n                 metrics=[tf.keras.metrics.RootMeanSquaredError()],\n                 )\n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# configure gpu\ngpus = tf.config.list_physical_devices('GPU'); \nif len(gpus) == 1:\n    strategy = tf.distribute.OneDeviceStrategy(device='/gpu:0')\nelif len(gpus) > 1:\n    strategy = tf.distribute.MirroredStrategy()\nelse:\n    strategy = tf.distribute.OneDeviceStrategy(device='/cpu:0')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ENABLE MIXED PRECISION for speed\n#tf.config.optimizer.set_jit(True)\ntf.config.optimizer.set_experimental_options({\"auto_mixed_precision\": True})\nprint('Mixed precision enabled')\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf.keras.backend.clear_session()\nwith strategy.scope():\n    model = get_model()\nmodel.summary()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 5 Folds Training Loop","metadata":{}},{"cell_type":"code","source":"valid_rmses = []\nfor fold in range(N_FOLD):\n    print(f'\\n-----------FOLD {fold} ------------')\n    \n    #Create dataset\n    train_df = df[df['fold'] != fold].reset_index(drop=True)\n    valid_df = df[df['fold'] == fold].reset_index(drop=True)\n    train_dataset = get_dataset(train_df)\n    valid_dataset = get_dataset(valid_df)\n    print('Data prepared.')\n    \n    #Create model\n    tf.keras.backend.clear_session()\n    with strategy.scope():\n        model = get_model()\n    print('Model prepared.')\n    \n    #Training model\n    print('Start training...')\n    callbacks = [\n    tf.keras.callbacks.ModelCheckpoint(f\"best_model_fold{fold}.h5\",\n                                       monitor=\"val_loss\",\n                                       mode=\"min\",\n                                       save_best_only=True,\n                                       verbose=1,\n                                       save_weights_only=True,),\n    tf.keras.callbacks.EarlyStopping(monitor='val_loss', \n                                     min_delta=1e-5, \n                                     patience=3, \n                                     verbose=1,\n                                     mode='min',)\n    ]\n    history = model.fit(x=train_dataset[0],\n                        y=train_dataset[1],\n                        validation_data=valid_dataset, \n                        epochs=10,\n                        shuffle=True,\n                        batch_size=BATCH_SIZE,\n                        verbose=1,\n                        callbacks=callbacks\n                       )\n    \n    valid_rmses.append(np.min(history.history['val_root_mean_squared_error']))\n    print('Training finished.')\n    del train_dataset, valid_dataset, train_df, valid_df\n    gc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f'{len(valid_rmses)} Folds validation RMSE:\\n{valid_rmses}')\nprint(f'Local CV Average score: {np.mean(valid_rmses)}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Inference and Submission\n\n","metadata":{}},{"cell_type":"code","source":"test_df = pd.read_csv('../input/feedback-prize-english-language-learning/test.csv')\ntest_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_dataset = deberta_encode(test_df['full_text'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 5 Folds ensemble prediction","metadata":{}},{"cell_type":"code","source":"fold_preds = []\nfor fold in range(N_FOLD):\n    tf.keras.backend.clear_session()\n    model = get_model()\n    model.load_weights(f'best_model_fold{fold}.h5')\n    print(f'\\nFold {fold} inference...')\n    pred = model.predict(test_dataset, batch_size=BATCH_SIZE)\n    fold_preds.append(pred)\n    gc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds = np.mean(fold_preds, axis=0)\npreds = np.clip(preds, 1, 5)\npreds = np.round(2*preds)/2\npreds","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub_df = pd.concat([test_df[['text_id']], pd.DataFrame(preds, columns=TARGET_COLS)], axis=1)\nsub_df.to_csv('submission.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}